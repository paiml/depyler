# Generated by: depyler corpus generator
# Source: hard_adv_parse_tokenizer.py
#
# Tokenizer that classifies integer-encoded characters into token types.
# Token types: 0=EOF, 1=NUMBER, 2=OPERATOR, 3=LPAREN, 4=RPAREN, 5=WHITESPACE, 6=UNKNOWN


def classify_char(ch: int) -> int:
    """Classify a character code into token type."""
    if ch >= 48 and ch <= 57:
        return 1
    if ch == 43 or ch == 45 or ch == 42 or ch == 47:
        return 2
    if ch == 40:
        return 3
    if ch == 41:
        return 4
    if ch == 32 or ch == 9 or ch == 10:
        return 5
    if ch == -1:
        return 0
    return 6


def scan_number(chars: list[int], start: int) -> tuple[int, int]:
    """Scan a number starting at position. Returns (value, end_pos)."""
    num: int = 0
    pos: int = start
    n: int = len(chars)
    while pos < n:
        ch: int = chars[pos]
        cls: int = classify_char(ch)
        if cls != 1:
            return (num, pos)
        num = num * 10 + (ch - 48)
        pos = pos + 1
    return (num, pos)


def skip_whitespace(chars: list[int], start: int) -> int:
    """Skip whitespace characters. Returns new position."""
    pos: int = start
    n: int = len(chars)
    while pos < n:
        ch: int = chars[pos]
        cls: int = classify_char(ch)
        if cls != 5:
            return pos
        pos = pos + 1
    return pos


def tokenize(chars: list[int]) -> list[int]:
    """Tokenize chars into flat list: [type1, val1, type2, val2, ...]."""
    result: list[int] = []
    pos: int = 0
    n: int = len(chars)
    while pos < n:
        pos = skip_whitespace(chars, pos)
        if pos >= n:
            return result
        ch: int = chars[pos]
        cls: int = classify_char(ch)
        if cls == 1:
            pair: tuple[int, int] = scan_number(chars, pos)
            result.append(1)
            result.append(pair[0])
            pos = pair[1]
        elif cls == 2:
            result.append(2)
            result.append(ch)
            pos = pos + 1
        elif cls == 3:
            result.append(3)
            result.append(40)
            pos = pos + 1
        elif cls == 4:
            result.append(4)
            result.append(41)
            pos = pos + 1
        else:
            result.append(6)
            result.append(ch)
            pos = pos + 1
    return result


def count_token_types(token_stream: list[int], target_kind: int) -> int:
    """Count tokens of a specific type in the stream."""
    cnt: int = 0
    i: int = 0
    n: int = len(token_stream)
    while i < n:
        tok_type: int = token_stream[i]
        if tok_type == target_kind:
            cnt = cnt + 1
        i = i + 2
    return cnt


def test_module() -> int:
    """Test tokenizer functions."""
    passed: int = 0

    c1: int = classify_char(50)
    if c1 == 1:
        passed = passed + 1

    c2: int = classify_char(43)
    if c2 == 2:
        passed = passed + 1

    pair: tuple[int, int] = scan_number([49, 50, 51, 43], 0)
    if pair[0] == 123:
        passed = passed + 1

    sp: int = skip_whitespace([32, 32, 49], 0)
    if sp == 2:
        passed = passed + 1

    toks: list[int] = tokenize([49, 43, 50])
    if len(toks) == 6:
        passed = passed + 1

    nums: int = count_token_types(toks, 1)
    if nums == 2:
        passed = passed + 1

    return passed
